{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e39063",
   "metadata": {},
   "source": [
    "# ADS 509 Module 3: Group Comparison \n",
    "\n",
    "The task of comparing two groups of text is fundamental to textual analysis. There are innumerable applications: survey respondents from different segments of customers, speeches by different political parties, words used in Tweets by different constituencies, etc. In this assignment you will build code to effect comparisons between groups of text data, using the ideas learned in reading and lecture.\n",
    "\n",
    "This assignment asks you to analyze the lyrics for the two artists you selected in Module 1 and the Twitter descriptions pulled for Robyn and Cher. If the results from that pull were not to your liking, you are welcome to use the zipped data from the â€œAssignment Materialsâ€ section. Specifically, you are asked to do the following: \n",
    "\n",
    "* Read in the data, normalize the text, and tokenize it. When you tokenize your Twitter descriptions, keep hashtags and emojis in your token set. \n",
    "* Calculate descriptive statistics on the two sets of lyrics and compare the results. \n",
    "* For each of the four corpora, find the words that are unique to that corpus. \n",
    "* Build word clouds for all four corpora. \n",
    "\n",
    "Each one of the analyses has a section dedicated to it below. Before beginning the analysis there is a section for you to read in the data and do your cleaning (tokenization and normalization). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e65f73",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abe420bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from wordcloud import WordCloud \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f064bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this space for any additional import statements you need\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "bcbe6342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place any addtional functions or constants you need here. \n",
    "\n",
    "# Some punctuation variations\n",
    "punctuation = set(punctuation) # speeds up comparison\n",
    "tw_punct = punctuation - {\"#\"}\n",
    "\n",
    "# Stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "# Two useful regex\n",
    "whitespace_pattern = re.compile(r\"\\s+\")\n",
    "hashtag_pattern = re.compile(r\"^#[0-9a-zA-Z]+\")\n",
    "\n",
    "# It's handy to have a full set of emojis\n",
    "all_language_emojis = set()\n",
    "\n",
    "for country in emoji.EMOJI_DATA : \n",
    "    for em in emoji.EMOJI_DATA[country] : \n",
    "        all_language_emojis.add(em)\n",
    "\n",
    "# and now our functions\n",
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity, and num_tokens most common\n",
    "        tokens. Return a list of \n",
    "    \"\"\"\n",
    "    # Place your Module 2 solution here\n",
    "    num_tokens = len(tokens)\n",
    "    num_unique_tokens = len(set(tokens))\n",
    "    if num_tokens > 0:\n",
    "        lexical_diversity = num_unique_tokens/num_tokens\n",
    "    else: lexical_diversity = 0\n",
    "    num_characters = sum(len(token) for token in tokens)\n",
    "    \n",
    "    count = Counter(tokens)\n",
    "    top_10 = count.most_common(10)\n",
    "\n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters, top_10])\n",
    "\n",
    "\n",
    "    \n",
    "def contains_emoji(s):\n",
    "    \n",
    "    s = str(s)\n",
    "    emojis = [ch for ch in s if emoji.is_emoji(ch)]\n",
    "\n",
    "    return(len(emojis) > 0)\n",
    "\n",
    "\n",
    "def remove_stop(tokens) :\n",
    "    # modify this function to remove stopwords\n",
    "    tokens = [word for word in tokens if word not in sw]\n",
    "    return(tokens)\n",
    " \n",
    "def remove_punctuation(text, punct_set=tw_punct) : \n",
    "    return(\"\".join([ch for ch in text if ch not in punct_set]))\n",
    "\n",
    "def tokenize(text) : \n",
    "    \"\"\" Splitting on whitespace rather than the book's tokenize function. That \n",
    "        function will drop tokens like '#hashtag' or '2A', which we need for Twitter. \"\"\"\n",
    "    # Convert to lowercase and split into words\n",
    "    # modify this function to return tokens\n",
    "    words = str(text).lower().split()\n",
    "    return(words)\n",
    "\n",
    "def prepare(text, pipeline) : \n",
    "    tokens = str(text)\n",
    "    \n",
    "    for transform in pipeline : \n",
    "        tokens = transform(tokens)\n",
    "        \n",
    "    return(tokens)\n",
    "\n",
    "def clean(data, col, sw):\n",
    "    text = ' '.join(data[col])\n",
    "    # Remove punctuation\n",
    "    text = re.sub(re.escape(str(tw_punct)), '', text)\n",
    "    # Convert to lowercase and split into words\n",
    "    words = str(text).lower().split()\n",
    "    # Remove stopwords\n",
    "    cleaned_tokens = [word for word in words if word not in sw]\n",
    "    return cleaned_tokens\n",
    "\n",
    "def sort(data, sort_col, comp_col):\n",
    "    data = data[data[sort_col] > data[comp_col]]\n",
    "    data = data.sort_values(by=sort_col, ascending=False).head(10)\n",
    "    return data[['token', sort_col, comp_col]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47735524",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "Use this section to ingest your data into the data structures you plan to use. Typically this will be a dictionary or a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff88201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to use the below cells as an example or read in the data in a way you prefer\n",
    "\n",
    "data_location = \"/Users/mtc/ADS/ADS 509/M1 Results/\" # change to your location if it is not in the same directory as your notebook\n",
    "twitter_folder = \"twitter/\"\n",
    "lyrics_folder = \"lyrics/\"\n",
    "\n",
    "artist_files = {'cher':'cher_followers_data.txt',\n",
    "                'robyn':'robynkonichiwa_followers_data.txt'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df415d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data = pd.read_csv(data_location + twitter_folder + artist_files['cher'],\n",
    "                           sep=\"\\t\",\n",
    "                           quoting=3)\n",
    "\n",
    "twitter_data['artist'] = \"cher\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "966804cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data_2 = pd.read_csv(data_location + twitter_folder + artist_files['robyn'],\n",
    "                             sep=\"\\t\",\n",
    "                             quoting=3)\n",
    "twitter_data_2['artist'] = \"robyn\"\n",
    "\n",
    "twitter_data = pd.concat([\n",
    "    twitter_data,twitter_data_2])\n",
    "    \n",
    "del(twitter_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "674767d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the lyrics here\n",
    "lyrics_data = []\n",
    "\n",
    "for artist in os.listdir(os.path.join(data_location,lyrics_folder)):\n",
    "    path = os.path.join(data_location,lyrics_folder, artist)\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "\n",
    "        song_path = path + '/' + file\n",
    "\n",
    "        with open(song_path, 'r', encoding='utf-8') as s:\n",
    "            lyrics = s.read()\n",
    "            title = str(re.findall(r'\".+\"', lyrics))\n",
    "            title = title[3:-3]\n",
    "            lyrics_c = re.sub('^\".+\"', '', lyrics)\n",
    "            lyrics_data.append({\n",
    "                'artist' :artist,\n",
    "                'song_title': title,\n",
    "                'lyrics': lyrics_c\n",
    "            })\n",
    "lyrics_data = pd.DataFrame(lyrics_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9892d14",
   "metadata": {},
   "source": [
    "## Tokenization and Normalization\n",
    "\n",
    "In this next section, tokenize and normalize your data. We recommend the following cleaning. \n",
    "\n",
    "**Lyrics** \n",
    "\n",
    "* Remove song titles\n",
    "* Casefold to lowercase\n",
    "* Remove stopwords (optional)\n",
    "* Remove punctuation\n",
    "* Split on whitespace\n",
    "\n",
    "Removal of stopwords is up to you. Your descriptive statistic comparison will be different if you include stopwords, though TF-IDF should still find interesting features for you. Note that we remove stopwords before removing punctuation because the stopword set includes punctuation.\n",
    "\n",
    "**Twitter Descriptions** \n",
    "\n",
    "* Casefold to lowercase\n",
    "* Remove stopwords\n",
    "* Remove punctuation other than emojis or hashtags\n",
    "* Split on whitespace\n",
    "\n",
    "Removing stopwords seems sensible for the Twitter description data. Remember to leave in emojis and hashtags, since you analyze those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5ca379eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the `pipeline` techniques from BTAP Ch 1 or 5\n",
    "\n",
    "my_pipeline = [str.lower, remove_punctuation, tokenize, remove_stop]\n",
    "\n",
    "lyrics_data[\"tokens\"] = lyrics_data[\"lyrics\"].apply(prepare,pipeline=my_pipeline)\n",
    "lyrics_data[\"num_tokens\"] = lyrics_data[\"tokens\"].map(len) \n",
    "\n",
    "twitter_data[\"tokens\"] = twitter_data[\"description\"].apply(prepare,pipeline=my_pipeline)\n",
    "twitter_data[\"num_tokens\"] = twitter_data[\"tokens\"].map(len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6cf534be",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data['has_emoji'] = twitter_data[\"description\"].apply(contains_emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec69ac9",
   "metadata": {},
   "source": [
    "Let's take a quick look at some descriptions with emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0a5a0512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "artist",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "description",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tokens",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "1e338f0f-24ec-487b-a078-ad1d85c47a05",
       "rows": [
        [
         "2548785",
         "cher",
         "Venezolana ğŸŒŸ Aficionada a la fotografÃ­a, redes sociales y viajar. Instagram @yosoybaniz",
         "['venezolana', 'ğŸŒŸ', 'aficionada', 'la', 'fotografÃ­a', 'redes', 'sociales', 'viajar', 'instagram', 'yosoybaniz']"
        ],
        [
         "1520851",
         "cher",
         "LoveDanceğŸ’",
         "['lovedanceğŸ’']"
        ],
        [
         "2788691",
         "cher",
         "ğŸ†",
         "['ğŸ†']"
        ],
        [
         "2318249",
         "cher",
         "ğŸŒ² loving, rescue ğŸ¶ advocate ğŸ’‡ğŸ¼, cancer survivor, ğŸ³ï¸â€ğŸŒˆ member, mental health sufferer believing in equality compassion and kindness â˜®ï¸",
         "['ğŸŒ²', 'loving', 'rescue', 'ğŸ¶', 'advocate', 'ğŸ’‡ğŸ¼', 'cancer', 'survivor', 'ğŸ³ï¸\\u200dğŸŒˆ', 'member', 'mental', 'health', 'sufferer', 'believing', 'equality', 'compassion', 'kindness', 'â˜®ï¸']"
        ],
        [
         "1278749",
         "cher",
         "#BLM, #climatechange, #LGBT ğŸŒˆ love animals am a fierce cook and Gardener Recovery for 12yrs y'all say hi",
         "['#blm', '#climatechange', '#lgbt', 'ğŸŒˆ', 'love', 'animals', 'fierce', 'cook', 'gardener', 'recovery', '12yrs', 'yall', 'say', 'hi']"
        ],
        [
         "1353614",
         "cher",
         "keep an open mind ğŸƒ",
         "['keep', 'open', 'mind', 'ğŸƒ']"
        ],
        [
         "676570",
         "cher",
         "#1 IASF stan. #1 Sweetener stanğŸ˜Œ| ig- swtoutsold",
         "['#1', 'iasf', 'stan', '#1', 'sweetener', 'stanğŸ˜Œ', 'ig', 'swtoutsold']"
        ],
        [
         "284418",
         "cher",
         "ğŸ³ï¸â€ğŸŒˆ she/her - IG: sarah.smjw",
         "['ğŸ³ï¸\\u200dğŸŒˆ', 'sheher', 'ig', 'sarahsmjw']"
        ],
        [
         "2804",
         "cher",
         "ğŸ“–ğŸ¤“",
         "['ğŸ“–ğŸ¤“']"
        ],
        [
         "105523",
         "cher",
         "Fun loving caring one Ina million ğŸ‡¯ğŸ‡²â™‹ï¸ğŸ‡¯ğŸ‡²â™‹ï¸ #MANCHESTERUNITEDFOREVER",
         "['fun', 'loving', 'caring', 'one', 'ina', 'million', 'ğŸ‡¯ğŸ‡²â™‹ï¸ğŸ‡¯ğŸ‡²â™‹ï¸', '#manchesterunitedforever']"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>description</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2548785</th>\n",
       "      <td>cher</td>\n",
       "      <td>Venezolana ğŸŒŸ Aficionada a la fotografÃ­a, redes...</td>\n",
       "      <td>[venezolana, ğŸŒŸ, aficionada, la, fotografÃ­a, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520851</th>\n",
       "      <td>cher</td>\n",
       "      <td>LoveDanceğŸ’</td>\n",
       "      <td>[lovedanceğŸ’]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2788691</th>\n",
       "      <td>cher</td>\n",
       "      <td>ğŸ†</td>\n",
       "      <td>[ğŸ†]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2318249</th>\n",
       "      <td>cher</td>\n",
       "      <td>ğŸŒ² loving, rescue ğŸ¶ advocate ğŸ’‡ğŸ¼, cancer survivo...</td>\n",
       "      <td>[ğŸŒ², loving, rescue, ğŸ¶, advocate, ğŸ’‡ğŸ¼, cancer, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278749</th>\n",
       "      <td>cher</td>\n",
       "      <td>#BLM, #climatechange, #LGBT ğŸŒˆ love animals am ...</td>\n",
       "      <td>[#blm, #climatechange, #lgbt, ğŸŒˆ, love, animals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353614</th>\n",
       "      <td>cher</td>\n",
       "      <td>keep an open mind ğŸƒ</td>\n",
       "      <td>[keep, open, mind, ğŸƒ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676570</th>\n",
       "      <td>cher</td>\n",
       "      <td>#1 IASF stan. #1 Sweetener stanğŸ˜Œ| ig- swtoutsold</td>\n",
       "      <td>[#1, iasf, stan, #1, sweetener, stanğŸ˜Œ, ig, swt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284418</th>\n",
       "      <td>cher</td>\n",
       "      <td>ğŸ³ï¸â€ğŸŒˆ she/her - IG: sarah.smjw</td>\n",
       "      <td>[ğŸ³ï¸â€ğŸŒˆ, sheher, ig, sarahsmjw]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2804</th>\n",
       "      <td>cher</td>\n",
       "      <td>ğŸ“–ğŸ¤“</td>\n",
       "      <td>[ğŸ“–ğŸ¤“]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105523</th>\n",
       "      <td>cher</td>\n",
       "      <td>Fun loving caring one Ina million ğŸ‡¯ğŸ‡²â™‹ï¸ğŸ‡¯ğŸ‡²â™‹ï¸ #MA...</td>\n",
       "      <td>[fun, loving, caring, one, ina, million, ğŸ‡¯ğŸ‡²â™‹ï¸ğŸ‡¯...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        artist                                        description  \\\n",
       "2548785   cher  Venezolana ğŸŒŸ Aficionada a la fotografÃ­a, redes...   \n",
       "1520851   cher                                         LoveDanceğŸ’   \n",
       "2788691   cher                                                  ğŸ†   \n",
       "2318249   cher  ğŸŒ² loving, rescue ğŸ¶ advocate ğŸ’‡ğŸ¼, cancer survivo...   \n",
       "1278749   cher  #BLM, #climatechange, #LGBT ğŸŒˆ love animals am ...   \n",
       "1353614   cher                                keep an open mind ğŸƒ   \n",
       "676570    cher   #1 IASF stan. #1 Sweetener stanğŸ˜Œ| ig- swtoutsold   \n",
       "284418    cher                      ğŸ³ï¸â€ğŸŒˆ she/her - IG: sarah.smjw   \n",
       "2804      cher                                                 ğŸ“–ğŸ¤“   \n",
       "105523    cher  Fun loving caring one Ina million ğŸ‡¯ğŸ‡²â™‹ï¸ğŸ‡¯ğŸ‡²â™‹ï¸ #MA...   \n",
       "\n",
       "                                                    tokens  \n",
       "2548785  [venezolana, ğŸŒŸ, aficionada, la, fotografÃ­a, re...  \n",
       "1520851                                       [lovedanceğŸ’]  \n",
       "2788691                                                [ğŸ†]  \n",
       "2318249  [ğŸŒ², loving, rescue, ğŸ¶, advocate, ğŸ’‡ğŸ¼, cancer, s...  \n",
       "1278749  [#blm, #climatechange, #lgbt, ğŸŒˆ, love, animals...  \n",
       "1353614                              [keep, open, mind, ğŸƒ]  \n",
       "676570   [#1, iasf, stan, #1, sweetener, stanğŸ˜Œ, ig, swt...  \n",
       "284418                       [ğŸ³ï¸â€ğŸŒˆ, sheher, ig, sarahsmjw]  \n",
       "2804                                                  [ğŸ“–ğŸ¤“]  \n",
       "105523   [fun, loving, caring, one, ina, million, ğŸ‡¯ğŸ‡²â™‹ï¸ğŸ‡¯...  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data[twitter_data.has_emoji].sample(10)[[\"artist\",\"description\",\"tokens\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2c55c9",
   "metadata": {},
   "source": [
    "With the data processed, we can now start work on the assignment questions. \n",
    "\n",
    "Q: What is one area of improvement to your tokenization that you could theoretically carry out? (No need to actually do it; let's not make perfect the enemy of good enough.)\n",
    "\n",
    "A: I think the tokenization could benefit from removing non-English stopwords. At the top of this assignment, the stopwords pulled in were specifically the english ones. If a twitter description or set of lyrics has words that aren't in English, the current code cannot remove stopwords in those instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1594271",
   "metadata": {},
   "source": [
    "## Calculate descriptive statistics on the two sets of lyrics and compare the results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "dc25e937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyrics- Robyn: Number of Tokens, Unique Tokens, Lexical Diversity, Number of Characters\n",
      "[14602, 2643, 0.18100260238323518, 73854, [('know', 297), ('love', 243), ('got', 240), ('like', 223), ('baby', 174), ('never', 148), ('dance', 143), ('get', 138), ('beat', 137), ('killing', 136)]]\n",
      "Lyrics- Cher: Number of Tokens, Unique Tokens, Lexical Diversity, Number of Characters \n",
      "[33711, 4592, 0.13621666518347128, 168274, [('love', 893), ('know', 448), ('time', 299), ('see', 286), ('one', 267), ('like', 259), ('come', 248), ('take', 248), ('go', 247), ('never', 246)]]\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "r_lyrics = clean(lyrics_data[lyrics_data['artist']=='robyn'], 'lyrics', sw)\n",
    "c_lyrics = clean(lyrics_data[lyrics_data['artist']=='cher'], 'lyrics', sw)\n",
    "\n",
    "print('Lyrics- Robyn: Number of Tokens, Unique Tokens, Lexical Diversity, Number of Characters\\n' + str(descriptive_stats(r_lyrics, verbose=True)))\n",
    "print('Lyrics- Cher: Number of Tokens, Unique Tokens, Lexical Diversity, Number of Characters \\n' + str(descriptive_stats(c_lyrics, verbose=True)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2ada9",
   "metadata": {},
   "source": [
    "Q: what observations do you make about these data? \n",
    "\n",
    "A: Cher has a higher number of characters, overall tokens, and a higher amount of unique tokens. This would make sense, since Cher's music career is quite long. Robyn's lyrics are more lexically diverse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750aa526",
   "metadata": {},
   "source": [
    "## Find tokens uniquely related to a corpus\n",
    "\n",
    "Typically we would use TF-IDF to find unique tokens in documents. Unfortunately, we either have too few documents (if we view each data source as a single document) or too many (if we view each description as a separate document). In the latter case, our problem will be that descriptions tend to be short, so our matrix would be too sparse to support analysis. \n",
    "\n",
    "To avoid these problems, we will create a custom statistic to identify words that are uniquely related to each corpus. The idea is to find words that occur often in one corpus and infrequently in the other(s). Since corpora can be of different lengths, we will focus on the _concentration_ of tokens within a corpus. \"Concentration\" is simply the count of the token divided by the total corpus length. For instance, if a corpus had length 100,000 and a word appeared 1,000 times, then the concentration would be $\\frac{1000}{100000} = 0.01$. If the same token had a concentration of $0.005$ in another corpus, then the concentration ratio would be $\\frac{0.01}{0.005} = 2$. Very rare words can easily create infinite ratios, so you will also add a cutoff to your code so that a token must appear at least $n$ times for you to return it. \n",
    "\n",
    "An example of these calculations can be found in [this spreadsheet](https://docs.google.com/spreadsheets/d/1P87fkyslJhqXFnfYezNYrDrXp_GS8gwSATsZymv-9ms). Please don't hesitate to ask questions if this is confusing. \n",
    "\n",
    "In this section find 10 tokens for each of your four corpora that meet the following criteria: \n",
    "\n",
    "1. The token appears at least `n` times in all corpora\n",
    "1. The tokens are in the top 10 for the highest ratio of appearances in a given corpora vs appearances in other corpora.\n",
    "\n",
    "You will choose a cutoff for yourself based on the side of the corpus you're working with. If you're working with the Robyn-Cher corpora provided, `n=5` seems to perform reasonably well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce72f0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "def get_count(tokens):\n",
    "    count = Counter(tokens)\n",
    "    df = pd.DataFrame.from_dict(count, orient = 'index').reset_index()\n",
    "    df = df.rename(columns={'index':'token', 0:'count'})\n",
    "    df['token']=df['token'].str.replace(',', '')\n",
    "    df['concentration'] = df['count']/sum(df['count'])\n",
    "    return(df[df['count']> 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "ac70538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_corpus = get_count(r_lyrics)\n",
    "r_corpus.rename(columns={'token':'token', 'count': 'count_r', 'concentration':'concentration_r'})\n",
    "\n",
    "c_corpus = get_count(c_lyrics)\n",
    "c_corpus.rename(columns={'token':'token', 'count': 'count_c', 'concentration':'concentration_c'})\n",
    "\n",
    "comp = pd.merge(r_corpus, c_corpus, on=['token'], how='inner' )\n",
    "comp['ratio_r_over_c'] = comp['concentration_x']/comp['concentration_y']\n",
    "comp['ratio_c_over_r'] = comp['concentration_y']/comp['concentration_x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08ecf82",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[209], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m r_twitter \u001b[38;5;241m=\u001b[39m \u001b[43mclean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtwitter_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtwitter_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43martist\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrobyn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m c_twitter \u001b[38;5;241m=\u001b[39m clean(twitter_data[twitter_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124martist\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcher\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m, sw)\n",
      "Cell \u001b[0;32mIn[197], line 78\u001b[0m, in \u001b[0;36mclean\u001b[0;34m(data, col, sw)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclean\u001b[39m(data, col, sw):\n\u001b[0;32m---> 78\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# Remove punctuation\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(re\u001b[38;5;241m.\u001b[39mescape(\u001b[38;5;28mstr\u001b[39m(tw_punct)), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "r_twitter = clean(twitter_data[twitter_data['artist'] == 'robyn'], \"description\", sw)\n",
    "c_twitter = clean(twitter_data[twitter_data['artist'] == 'cher'], 'description', sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "c8118fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with Higher Ratio in Robyn lyrics than Cher Lyrics \n",
      "      token  ratio_r_over_c  ratio_c_over_r\n",
      "137   like       64.353796        0.015539\n",
      "88   right       43.864471        0.022797\n",
      "135   want       41.940590        0.023843\n",
      "53    know       28.569622        0.035002\n",
      "58   thing       24.240892        0.041253\n",
      "14    beat       21.085728        0.047425\n",
      "127   back       18.469251        0.054144\n",
      "4     time       17.403717        0.057459\n",
      "167  dance       15.006266        0.066639\n",
      "261   stop       12.202898        0.081948\n",
      "Words with Higher Ratio in Cher lyrics than Robyn Lyrics \n",
      "        token  ratio_c_over_r  ratio_r_over_c\n",
      "276     know       32.342045        0.030920\n",
      "355     love       32.233757        0.031023\n",
      "284      say       10.890688        0.091822\n",
      "254       go        9.726240        0.102815\n",
      "294     find        9.529352        0.104939\n",
      "206      man        7.599855        0.131581\n",
      "238    right        6.822150        0.146581\n",
      "225   enough        6.187891        0.161606\n",
      "115  believe        6.016005        0.166223\n",
      "97        oh        5.661920        0.176619\n"
     ]
    }
   ],
   "source": [
    "print('Words with Higher Ratio in Robyn lyrics than Cher Lyrics \\n', sort(comp, 'ratio_r_over_c', 'ratio_c_over_r'))\n",
    "print('Words with Higher Ratio in Cher lyrics than Robyn Lyrics \\n', sort(comp, 'ratio_c_over_r', 'ratio_r_over_c'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53526fcd",
   "metadata": {},
   "source": [
    "Q: What are some observations about the top tokens? Do you notice any interesting items on the list? \n",
    "\n",
    "A: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4f52b3",
   "metadata": {},
   "source": [
    "## Build word clouds for all four corpora. \n",
    "\n",
    "For building wordclouds, we'll follow exactly the code of the text. The code in this section can be found [here](https://github.com/blueprints-for-text-analytics-python/blueprints-text/blob/master/ch01/First_Insights.ipynb). If you haven't already, you should absolutely clone the repository that accompanies the book. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786b2af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def wordcloud(word_freq, title=None, max_words=200, stopwords=None):\n",
    "\n",
    "    wc = WordCloud(width=800, height=400, \n",
    "                   background_color= \"black\", colormap=\"Paired\", \n",
    "                   max_font_size=150, max_words=max_words)\n",
    "    \n",
    "    # convert data frame into dict\n",
    "    if type(word_freq) == pd.Series:\n",
    "        counter = Counter(word_freq.fillna(0).to_dict())\n",
    "    else:\n",
    "        counter = word_freq\n",
    "\n",
    "    # filter stop words in frequency counter\n",
    "    if stopwords is not None:\n",
    "        counter = {token:freq for (token, freq) in counter.items() \n",
    "                              if token not in stopwords}\n",
    "    wc.generate_from_frequencies(counter)\n",
    " \n",
    "    plt.title(title) \n",
    "\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    \n",
    "def count_words(df, column='tokens', preprocess=None, min_freq=2):\n",
    "\n",
    "    # process tokens and update counter\n",
    "    def update(doc):\n",
    "        tokens = doc if preprocess is None else preprocess(doc)\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # create counter and run through all data\n",
    "    counter = Counter()\n",
    "    df[column].map(update)\n",
    "\n",
    "    # transform counter into data frame\n",
    "    freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])\n",
    "    freq_df = freq_df.query('freq >= @min_freq')\n",
    "    freq_df.index.name = 'token'\n",
    "    \n",
    "    return freq_df.sort_values('freq', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a2e53",
   "metadata": {},
   "source": [
    "Q: What observations do you have about these (relatively straightforward) wordclouds? \n",
    "\n",
    "A: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
